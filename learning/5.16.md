# 5/16 progress:

Today, I worked on backend development of translation functionality of Python application

Firstly, I learned how to set up the development environment, installing necessary Python packages 
such as `speech_recognition`, `requests`, and `azure-cognitiveservices-speech`, and configured the 
environment to include these libraries. 

### **Current library functions:**

**import speech_recognition:**

The speech recognition library is used for recognizing speech in audio recordings. It provides simple interfaces for various speech recognition APIs. It can capture audio from a microphone or audio file, recognize and convert spoken language into text using various APIs, and handle ambient noise to improve the accuracy of speech recognition. 

The Recognizer class serves to recognize speech, and each Recognizer instance comes with a variety of settings and functionality for recognizing speech from an audio source. 

For instance: 
recognizer.recognize_google(audio, language='yue') accesses the recognize_google() Google Web Speech API in order to recognize speech from an audio source. The recognize_() methods of the Recognizer class also require arguments to perform speech recognition on, which include audio_data.

with sr.Microphone() as mic: 
            recognizer.adjust_for_ambient_noise(mic, duration=0.2) # handle ambient noise
            audio = recognizer.listen(mic) # records a single phrase from mic (an AudioSource instance) into an AudioData instance
            text = recognizer.recognize_google(audio, language='yue') 
            text = text.lower() # makes outputted text lowercase
            print(f"Recognized: {text}")
            
The Microphone class is given from the pyaudio library, which is installed onto terminal. Thus, instead of using an audio file as the source, which is the default of the speech recognition library, we can use the default system microphone, which can be accssed by creating an instance of the Microphone class. 



import os




import requests
import uuid
import azure.cognitiveservices.speech as speechsdk



Secondly, I implemented the speech recognition functionality by utilizing the `speech_recognition` 
library to capture audio from the microphone, adjusting for ambient noise to improve recognition accuracy. 
I then used Google's speech recognition service to convert spoken Chinese (Mandarin) into text. 

Through this process, I learned how to capture and process audio input in Python and use external APIs for 
speech recognition. Additionally, I integrated the Microsoft Translator API to handle text translation from 
Mandarin to Cantonese, which involved understanding API request structures and handling JSON responses. 

I also explored Azure Cognitive Services' Speech SDK for converting the translated text into speech, learning how to
synthesize audio output and handle exceptions. Furthermore, I gained some insight into incorporating Large 
Language Models (LLMs) into projects, which enhanced my understanding of using advanced AI capabilities for
language processing tasks. 

This experience has significantly broadened my skills in working with APIs, managing 
audio input and output, and integrating complex AI models into applications.
