# 5/16 progress:

Today, I worked on backend development of translation functionality of Python application

First, I learned how to set up the development environment, installing necessary Python packages 
such as `speech_recognition`, `requests`, and `azure-cognitiveservices-speech`, and configured the 
environment to include these libraries. 

### **Current library functions:**

**import speech_recognition:**

The speech recognition library is used for recognizing speech in audio recordings. It provides simple interfaces for various speech recognition APIs. It can capture audio from a microphone or audio file, recognize and convert spoken language into text using various APIs, and handle ambient noise to improve the accuracy of speech recognition. 

The Recognizer class serves to recognize speech, and each Recognizer instance comes with a variety of settings and functionality for recognizing speech from an audio source. 

For instance: 
      *recognizer.recognize_google(audio, language='yue')*
accesses the recognize_google() Google Web Speech API in order to recognize speech from an audio source. The recognize_() methods of the Recognizer class also require an audio_data argument. Audio_data must be an instance of Speech Recognition's AudioData class.

The two ways to create an AudioData instance: from an audio file or audio recorded by a microphone. To initiate the microphone:

    with sr.Microphone() as mic:
                recognizer.adjust_for_ambient_noise(mic, duration=0.2)
                audio = recognizer.listen(mic)
                text = recognizer.recognize_google(audio, language='yue')
                text = text.lower()
                print(f"Recognized: {text}")

                              
The Microphone class is given from the pyaudio library, which is installed onto terminal. Thus, instead of using an audio file as the source, which is the default of the speech recognition library, we can use the default system microphone, which can be accssed by creating an instance of the Microphone class. 

The rest of the documentation for Speech Recognition can be found [here](https://github.com/Uberi/speech_recognition/blob/master/reference/library-reference.rst). 

**import os**

The OS module in Python provides functions for interacting with the operating system. Key functionalities include interacting with the file system (creating, removing, and renaming files and directories), accessing environment variables, running shell commands and scripts, and managing file paths and navigating the directory structure. 

     temp_file = synthesize_speech(translated_text)
                 if temp_file:
                     print("Speech synthesized to file:", temp_file)
                     os.remove(temp_file)
                     
After synthesizing speech from the translated text, a temp_file (temp.wav) for that audio_data is created and then removed by os.remove(temp_file) afterwards. 

**import requests**

The requests library is used for making HTTP requests to interact with web services and APIs. The HTTP request returns a Response Object with all the response data (content, encoding, status, etc). 

     body = [{'text': text}] # creates the request payload to be sent to the translation API
         response = requests.post(url, params=params, headers=headers, json=body) 
         translated_text = response.json()[0]['translations'][0]['text'] # extracts translated text from JSON response
         return translated_text # returns the extracted translated text to the caller of the function
         
request(method, url, args) sends a request of the specified [method](https://www.w3schools.com/python/ref_requests_response.asp) to the specified url. In the requests line above, The url is the endpoint URL where the request is being sent. Params is a dictionary of URL parameters to be appended to the URL(API version, source, and target languages). Headers is a dictionary of HTTP headers to include in the request (API subscription, key, region, etc.) Json is the payload to be sent in the request body, automatically serialized to JSON format by the requests library. 

The requests.post() method sends the HTTP POST request with the given URL, parameters, headers, and body, and returns a response object containing the server's response to the request (which returns as a JSON response from the API). 

**import uuid**

The uuid module is used for generating universally unique identifiers (UUIDs), which are used as identifiers in databases, APIs, and other systems. 

**import azure.cognitiveservices.speech as speechsdk**

This module is part of the Azure Cognitive Services and is used for speech-related functionalities, such as speech recognition, speech synthesis (text-to-speech), and speech translation. 

     def synthesize_speech(text, language='yue'):
         try:
             speech_config = speechsdk.SpeechConfig(subscription=subscription_key, region=region)
             speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)
             result = speech_synthesizer.speak_text_async(text).get()
             if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                 filename = "temp.wav"
                 with open(filename, "wb") as audio_file:
                     audio_file.write(result.audio_data)
                 return filename
             else:
                 print("Speech synthesis failed: {}".format(result.reason))
                 return None
         except Exception as e:
             print("Error during speech synthesis:", e)
             return None

Through the library, SpeechConfig is aclass that defines configurations for speech / intent recognition and speech synthesis. The configuration can be initialized in different ways, including from subscription (pass a subscription key and a region). More about the module can be found [here](https://learn.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech?view=azure-python). 

Secondly, I implemented the speech recognition functionality by utilizing the `speech_recognition` 
library to capture audio from the microphone, adjusting for ambient noise to improve recognition accuracy. 
I then used Google's speech recognition service to convert spoken Chinese (Mandarin) into text. 

Through this process, I learned how to capture and process audio input in Python and use external APIs for 
speech recognition. Additionally, I integrated the Microsoft Translator API to handle text translation from 
Mandarin to Cantonese, which involved understanding API request structures and handling JSON responses. 

I also explored Azure Cognitive Services' Speech SDK for converting the translated text into speech, learning how to
synthesize audio output and handle exceptions. Furthermore, I gained some insight into incorporating Large 
Language Models (LLMs) into projects, which enhanced my understanding of using advanced AI capabilities for
language processing tasks. 

This experience has significantly broadened my skills in working with APIs, managing 
audio input and output, and integrating complex AI models into applications.
